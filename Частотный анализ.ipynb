{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hTjyYY0pfZkc"
   },
   "source": [
    "Статьи для чтения\n",
    "*   [~~Частотный анализ~~](https://habr.com/ru/articles/583304/)\n",
    "*   [Необоснованная эффективность рекуррентных нейронных сетей](https://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "*   [Fun with RNNs](https://www.cs.toronto.edu/~guerzhoy/321/proj4/)\n",
    "*   http://shannon.usu.edu.ru/Shannon/shannon1948/node2.html\n",
    "*   [Рекуррентные нейронные сети (RNN) с Keras](https://habr.com/ru/articles/487808/)\n",
    "* https://www.tensorflow.org/text/tutorials/text_generation?hl=ru\n",
    "* https://skine.ru/articles/734433/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5TB-c121TaNF"
   },
   "source": [
    "##Частотный анализ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-05-14T14:57:22.089941Z",
     "iopub.status.busy": "2024-05-14T14:57:22.089221Z",
     "iopub.status.idle": "2024-05-14T14:57:38.306611Z",
     "shell.execute_reply": "2024-05-14T14:57:38.305864Z",
     "shell.execute_reply.started": "2024-05-14T14:57:22.089899Z"
    },
    "executionInfo": {
     "elapsed": 24293,
     "status": "ok",
     "timestamp": 1709381963940,
     "user": {
      "displayName": "Дмитрий Александров",
      "userId": "00273293324083481877"
     },
     "user_tz": -180
    },
    "id": "1v-KcnohIENL",
    "outputId": "6ff0eb05-62b0-449b-d7c7-fba8d781cde4",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: frequency-analysis in /home/jupyter/.local/lib/python3.10/site-packages (0.1.4.5)\n",
      "Requirement already satisfied: xlsxwriter in /home/jupyter/.local/lib/python3.10/site-packages (from frequency-analysis) (3.2.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: beautifulsoup4 in /kernel/lib/python3.10/site-packages (4.12.3)\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (4.9.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /kernel/lib/python3.10/site-packages (from beautifulsoup4) (2.5)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.6)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install frequency-analysis\n",
    "%pip install beautifulsoup4 lxml\n",
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-14T14:57:42.801405Z",
     "iopub.status.busy": "2024-05-14T14:57:42.800788Z",
     "iopub.status.idle": "2024-05-14T14:57:46.341335Z",
     "shell.execute_reply": "2024-05-14T14:57:46.340693Z",
     "shell.execute_reply.started": "2024-05-14T14:57:42.801365Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import io\n",
    "import os\n",
    "import string\n",
    "from datetime import datetime\n",
    "from os import listdir\n",
    "import frequency_analysis\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-14T14:57:51.216024Z",
     "iopub.status.busy": "2024-05-14T14:57:51.215202Z",
     "iopub.status.idle": "2024-05-14T14:57:51.233624Z",
     "shell.execute_reply": "2024-05-14T14:57:51.232869Z",
     "shell.execute_reply.started": "2024-05-14T14:57:51.215992Z"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1709381968200,
     "user": {
      "displayName": "Дмитрий Александров",
      "userId": "00273293324083481877"
     },
     "user_tz": -180
    },
    "id": "nJaPqhM1LDS-",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clear_and_remove_dir(dir):\n",
    "    if os.path.exists(dir + '/.ipynb_checkpoints'):\n",
    "        os.rmdir(dir + '/.ipynb_checkpoints')\n",
    "    if os.path.exists(dir):\n",
    "        remove_files(dir)\n",
    "        os.rmdir(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-14T14:57:52.225870Z",
     "iopub.status.busy": "2024-05-14T14:57:52.225065Z",
     "iopub.status.idle": "2024-05-14T14:57:52.271013Z",
     "shell.execute_reply": "2024-05-14T14:57:52.270345Z",
     "shell.execute_reply.started": "2024-05-14T14:57:52.225836Z"
    },
    "executionInfo": {
     "elapsed": 419,
     "status": "ok",
     "timestamp": 1709381974492,
     "user": {
      "displayName": "Дмитрий Александров",
      "userId": "00273293324083481877"
     },
     "user_tz": -180
    },
    "id": "vwCOeLmZNu44",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_files(dir):\n",
    "    files = listdir(dir)\n",
    "    for n, file in enumerate(files):\n",
    "        os.remove(dir + '/' + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-14T14:57:53.101490Z",
     "iopub.status.busy": "2024-05-14T14:57:53.100781Z",
     "iopub.status.idle": "2024-05-14T14:57:53.160679Z",
     "shell.execute_reply": "2024-05-14T14:57:53.160023Z",
     "shell.execute_reply.started": "2024-05-14T14:57:53.101450Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_xls_report(dir, yo_enabled):\n",
    "    with frequency_analysis.Result(name = dir) as res:\n",
    "        res.treat(limits=(1000,) * 4, chart_limits=(20,) * 4, min_quantities=(10,) * 5)\n",
    "        res.sheet_ru_top_symbols()  # optional argument – chart_limit (int)\n",
    "        ru_symbs = 'абвгдежзийклмнопрстуфхцчшщьыъэюяz!?.,'\n",
    "        res.sheet_custom_symbol_bigrams(ru_symbs, ignore_case=True, name='Russian letter bigrams')\n",
    "        if yo_enabled:\n",
    "            res.sheet_yo_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-14T14:57:53.951009Z",
     "iopub.status.busy": "2024-05-14T14:57:53.950305Z",
     "iopub.status.idle": "2024-05-14T14:57:54.003448Z",
     "shell.execute_reply": "2024-05-14T14:57:54.002687Z",
     "shell.execute_reply.started": "2024-05-14T14:57:53.950977Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokinize(text):\n",
    "    return word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-14T14:57:54.794942Z",
     "iopub.status.busy": "2024-05-14T14:57:54.794213Z",
     "iopub.status.idle": "2024-05-14T14:57:54.813288Z",
     "shell.execute_reply": "2024-05-14T14:57:54.812591Z",
     "shell.execute_reply.started": "2024-05-14T14:57:54.794909Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clear_text(text):\n",
    "    spec_chars = '\\n\\xa0«»\\t—…{}[]()' \n",
    "    cleared = text.lower()\n",
    "    cleared = remove_chars_from_text(cleared, spec_chars)\n",
    "    cleared = remove_chars_from_text(cleared, string.digits)\n",
    "    return cleared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-14T14:57:55.622782Z",
     "iopub.status.busy": "2024-05-14T14:57:55.622006Z",
     "iopub.status.idle": "2024-05-14T14:57:55.657704Z",
     "shell.execute_reply": "2024-05-14T14:57:55.656979Z",
     "shell.execute_reply.started": "2024-05-14T14:57:55.622737Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_progress_to_file(n, length, name):\n",
    "    with open('progress.txt', 'a') as f:\n",
    "        print(n, \"/\", length, name, datetime.now().strftime('%H:%M:%S'), file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-14T14:57:57.893676Z",
     "iopub.status.busy": "2024-05-14T14:57:57.893022Z",
     "iopub.status.idle": "2024-05-14T14:57:57.970130Z",
     "shell.execute_reply": "2024-05-14T14:57:57.969332Z",
     "shell.execute_reply.started": "2024-05-14T14:57:57.893636Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_to_sentence(text):\n",
    "    return text.split(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-14T14:57:58.680144Z",
     "iopub.status.busy": "2024-05-14T14:57:58.679682Z",
     "iopub.status.idle": "2024-05-14T14:57:58.717820Z",
     "shell.execute_reply": "2024-05-14T14:57:58.717160Z",
     "shell.execute_reply.started": "2024-05-14T14:57:58.680113Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_chars_from_text(text, chars):\n",
    "    return \"\".join([ch for ch in text if ch not in chars])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-14T14:57:59.787930Z",
     "iopub.status.busy": "2024-05-14T14:57:59.787179Z",
     "iopub.status.idle": "2024-05-14T14:57:59.807189Z",
     "shell.execute_reply": "2024-05-14T14:57:59.806545Z",
     "shell.execute_reply.started": "2024-05-14T14:57:59.787890Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def xml_analysis(input_dir, output_dir, yo_enabled):\n",
    "    clear_and_remove_dir(output_dir)\n",
    "    file_list = listdir(input_dir)\n",
    "    start = datetime.now()\n",
    "    with frequency_analysis.Analysis(name = output_dir, yo=yo_enabled) as analyze:\n",
    "        for n, file in enumerate(file_list):\n",
    "            with io.open(input_dir + '/' + file, mode='r', encoding='utf-8') as f:\n",
    "                data = f.read()\n",
    "            bs_data = BeautifulSoup(input_dir, 'xml')\n",
    "            for sentence in bs_data.find_all('source'):\n",
    "                printsentence.text.split()\n",
    "                analyze.count_symbols(sentence.text.split(), pos=True)\n",
    "            print(n, file)\n",
    "    print('fin at:', datetime.now().strftime('%H:%M:%S'))\n",
    "    print('total time taked to analysis:', datetime.now() - start)\n",
    "    build_xls_report(output_dir, yo_enabled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-14T14:58:00.576253Z",
     "iopub.status.busy": "2024-05-14T14:58:00.575542Z",
     "iopub.status.idle": "2024-05-14T14:58:00.717325Z",
     "shell.execute_reply": "2024-05-14T14:58:00.716672Z",
     "shell.execute_reply.started": "2024-05-14T14:58:00.576209Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def txt_analysis(input_dir, output_dir, yo_enabled):\n",
    "    clear_and_remove_dir(output_dir)\n",
    "    file_list = listdir(input_dir)\n",
    "    word_pattern = '[a-zA-Zа-яА-ЯёЁ]+(?:(?:-?[a-zA-Zа-яА-ЯёЁ]+)+|\\'?[a-zA-Zа-яА-ЯёЁ]+)|[a-zA-Zа-яА-ЯёЁ]';\n",
    "    start = datetime.now()\n",
    "    with frequency_analysis.Analysis(name = output_dir, yo=yo_enabled, word_pattern=word_pattern) as analyze:\n",
    "        for n, file in enumerate(file_list):\n",
    "            print_progress_to_file(n, len(file_list), file)\n",
    "            with io.open(input_dir + '/' + file, mode='r', encoding='utf-8') as f:\n",
    "                try:\n",
    "                    text = f.read()\n",
    "                except Exception:\n",
    "                    print('Can not read file', f)\n",
    "                    continue\n",
    "            sentences = sent_tokenize(text)\n",
    "            for sentence in sentences:\n",
    "                if len(sentence) > 1:\n",
    "                    buffer = []\n",
    "                    for word in sentence.split():\n",
    "                        word = word + \"z\"\n",
    "                        buffer.append(word)\n",
    "                    analyze.count_symbols(buffer, pos=True)\n",
    "    print('fin at:', datetime.now().strftime('%H:%M:%S'))\n",
    "    print('total time taked to analysis:', datetime.now() - start)\n",
    "    build_xls_report(output_dir, yo_enabled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-14T14:58:03.257209Z",
     "iopub.status.busy": "2024-05-14T14:58:03.256515Z",
     "iopub.status.idle": "2024-05-14T14:58:03.286342Z",
     "shell.execute_reply": "2024-05-14T14:58:03.285592Z",
     "shell.execute_reply.started": "2024-05-14T14:58:03.257174Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def my_analysis(input_dir, output_dir, input_text_format, yo_enabled=False):\n",
    "    if(input_text_format == 'txt'):\n",
    "        txt_analysis(input_dir, output_dir, yo_enabled)\n",
    "    else:\n",
    "        xml_analysis(input_dir, output_dir, yo_enabled)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UM_ykSZ2Tfnb"
   },
   "source": [
    "##Анализ произведений Пушкина"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-14T14:58:15.639372Z",
     "iopub.status.busy": "2024-05-14T14:58:15.638680Z",
     "iopub.status.idle": "2024-05-14T14:58:25.301086Z",
     "shell.execute_reply": "2024-05-14T14:58:25.300309Z",
     "shell.execute_reply.started": "2024-05-14T14:58:15.639333Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fin at: 14:58:24\n",
      "total time taked to analysis: 0:00:08.810890\n",
      "Start of writing to .xlsx\n",
      "... \"Stats\" sheet was written\n",
      "... \"Top symbols\" sheet was written\n",
      "... \"Top symbol bigrams\" sheet was written\n",
      "... \"All symbol bigrams\" table sheet was written\n",
      "... \"Top words\" sheet was written\n",
      "... \"Top word bigrams\" sheet was written\n",
      "End of writing main sheets.\n",
      "You can call additional functions to create more sheets (e.g. \"sheet_en_symbol_bigrams()\", \"sheet_ru_symbol_bigrams()\", \"sheet_en_top_symbols([chart_limit])\", \"sheet_ru_top_symbols([chart_limit]), \"\"sheet_yo_words([limit, min_quantity])\"), \"sheet_custom_top_symbols(symbols_str)\" or \"sheet_custom_symbol_bigrams(symbols_str)\".\n",
      "You can also call 2D sheet functions with \"ignore_case=True\" argument.\n",
      "... \"Russian letters top\" sheet was written.\n",
      "... \"Russian letter bigrams (I)\" sheet was written.\n"
     ]
    }
   ],
   "source": [
    "my_analysis('test3', 'test3_analysis', 'txt', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T19:55:26.643134Z",
     "iopub.status.busy": "2024-05-10T19:55:26.642514Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "# remove_files('Lexica' + '/.ipynb_checkpoints')\n",
    "# os.rmdir('Lexica' + '/.ipynb_checkpoints')\n",
    "\n",
    "my_analysis(\"Lexica\", \"lexica_analysis\", \"txt\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of writing to .xlsx\n",
      "... \"Stats\" sheet was written\n",
      "... \"Top symbols\" sheet was written\n",
      "... \"Top symbol bigrams\" sheet was written\n",
      "... \"All symbol bigrams\" table sheet was written\n",
      "... \"Top words\" sheet was written\n",
      "... \"Top word bigrams\" sheet was written\n",
      "End of writing main sheets.\n",
      "You can call additional functions to create more sheets (e.g. \"sheet_en_symbol_bigrams()\", \"sheet_ru_symbol_bigrams()\", \"sheet_en_top_symbols([chart_limit])\", \"sheet_ru_top_symbols([chart_limit]), \"\"sheet_yo_words([limit, min_quantity])\"), \"sheet_custom_top_symbols(symbols_str)\" or \"sheet_custom_symbol_bigrams(symbols_str)\".\n",
      "You can also call 2D sheet functions with \"ignore_case=True\" argument.\n",
      "... \"Russian letters top\" sheet was written.\n",
      "... \"Russian letter bigrams (I)\" sheet was written.\n"
     ]
    }
   ],
   "source": [
    "build_xls_report(\"lexica_analysis\", False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyONnka8rFzDwmAYoDHXDQht",
   "mount_file_id": "1nfLF5jo4nOo0HCP2VZWLQUgslpObGzpl",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
